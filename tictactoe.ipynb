{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb53b1a",
   "metadata": {},
   "source": [
    "# Decision Theory Project - TicTacToe\n",
    "*By Jelle Huibregtse and Aron Hemmes*\n",
    "\n",
    "Below is a TicTacToe environment build from scratch with an Agent based on reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e2276",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "- Loading in some libraries\n",
    "- Configuring layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import random\n",
    "import functools\n",
    "from enum import Enum\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout, Button, HTML, Box\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Layout\n",
    "field_layout = Layout(width=\"50px\", height=\"50px\")\n",
    "wide_layout = Layout(width=\"158px\")\n",
    "column_layout = Layout(flex_flow=\"column\")\n",
    "\n",
    "# Formatting\n",
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf218cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".widget-button {\n",
    "    outline: none !important;\n",
    "}\n",
    "\n",
    ".widget-html-content {\n",
    "    white-space: pre-wrap;\n",
    "    line-height: normal !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344c8bd",
   "metadata": {},
   "source": [
    "## 2. Definition of the Environment\n",
    "\n",
    "The code below defines all characteristics of our tic-tac-toe environment with the following characteristics:\n",
    "\n",
    "Environment state:\n",
    "\n",
    "- the player type is either X or O\n",
    "- the opposing player (agent) is either X or O depending on the player\n",
    "- X and O take turns placing an X or O on empty fields untill either one has won or there are no more fields left on the board\n",
    "- a board starts out empty and can contain X and O marks\n",
    "\n",
    "A TicTacToeEnvironment object has the following methods:\n",
    "- `reset()` which completely resets the board to an empty state.\n",
    "- `update()` Updates the visualisation of the current TicTacToe game.\n",
    "- `render()` Visualisation of the current TicTacToe game.\n",
    "- `change_player()` The player switches between X and O and resets the board.\n",
    "- `field_click(field)` The player sets a field to a type if the field is None.\n",
    "\n",
    "Aditionally to allow an agent to calculate optimal decisions using model information, these methods are also available:\n",
    "- `get_turns()` Returns the amount of turns that have passed.\n",
    "- `get_active()` Returns which player's turn it currently is.\n",
    "- `get_state()` Returns a string representing the current board.\n",
    "- `get_reward(field)` Simplified version $R(s,a)$ of the general reward function: $R(s,â) = 1$, $R(s,ã) = -1$, $R(s,a) = 0$\n",
    "- `get_winnable_fields(player)` Returns all fields for a player that allow them to win. \n",
    "- `get_result()` Returns which player's got 3 in a row, otherwise returns None.\n",
    "- `set_field(field)` Sets a field to currently active player.\n",
    "- `step()` Processes the agent policies and returns the action, state and reward for the current active agent policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436da65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [[int(a), int(b), int(c)] for a, b, c in [\"036\", \"012\", \"147\", \"345\", \"048\", \"246\", \"678\", \"258\"]]\n",
    "\n",
    "class Type(Enum):\n",
    "    X = 1\n",
    "    O = 2\n",
    "\n",
    "class TicTacToeEnvironment:\n",
    "    def __init__(self, policies = None):\n",
    "        self.board = [None for _ in range(9)]\n",
    "        self.player = Type.X\n",
    "        self.policies = policies\n",
    "        \n",
    "        if not policies == None and not len(policies) > 1:\n",
    "            self.step()\n",
    "\n",
    "    def get_turns(self) -> int:\n",
    "        return len([field for field in self.board if not field == None])\n",
    "\n",
    "    def get_active(self) -> Type:\n",
    "        return Type.X if self.get_turns() % 2 == 0 else Type.O\n",
    "\n",
    "    def get_state(self) -> str:\n",
    "        return str([f.name if not f == None else 'E' for f in self.board]).replace('\\'', '')\n",
    "\n",
    "    def get_result(self) -> Type:\n",
    "        # Check for three of the same marks in a row\n",
    "        board = self.board\n",
    "        for row in rows:\n",
    "            # If not none and three in a row are the same\n",
    "            if not board[row[0]] == None and board[row[0]] == board[row[1]] == board[row[2]]:\n",
    "                # Return board\n",
    "                return board[row[0]]\n",
    "\n",
    "    def step(self) -> Tuple[Type, int, float, str]:\n",
    "        # Execute agent code\n",
    "        field = None\n",
    "        if self.get_result() == None:\n",
    "            if not self.policies == None:\n",
    "                if len(self.policies) == 1:\n",
    "                    if not self.player == self.get_active():\n",
    "                        field = self.policies[0](self)\n",
    "                if len(self.policies) == 2:\n",
    "                    if self.player == self.get_active():\n",
    "                        field = self.policies[0](self)\n",
    "                    else:\n",
    "                        field = self.policies[1](self)\n",
    "        \n",
    "        if not field == None:\n",
    "            player, reward = self.get_active(), self.get_reward(field)\n",
    "            self.set_field(field)\n",
    "            return player, field, reward, self.get_state()\n",
    "    \n",
    "    def get_reward(self, field: int) -> float:\n",
    "        player_fields = self.get_winnable_fields(self.get_active())\n",
    "        opposing_fields = self.get_winnable_fields(Type.O if self.get_active() == Type.X else Type.X)\n",
    "        # If player places in field with 3 in a row, return 1.0\n",
    "        if field in player_fields:\n",
    "            return 1.0\n",
    "        # If player doesn't place in field where opposing can get 3 in a row, return -1.0\n",
    "        elif len(opposing_fields) > 0 and not field in opposing_fields:\n",
    "            return -1.0\n",
    "        # Return 0.0 by default\n",
    "        return 0.0\n",
    "\n",
    "    def reset(self, _ = None) -> None:\n",
    "        self.board = [None for _ in range(9)]\n",
    "        self.update()\n",
    "        \n",
    "        if not policies == None and not len(policies) > 1:\n",
    "            self.step()\n",
    "\n",
    "    def change_player(self, _ = None) -> None:\n",
    "        self.player = Type.O if self.player == Type.X else Type.X\n",
    "        self.reset()\n",
    "\n",
    "    def field_click(self, field: int, _ = None) -> None:\n",
    "        if self.policies == None or not len(self.policies) > 1:\n",
    "            if self.get_result() == None and self.board[field] == None:\n",
    "                if self.policies == None or self.get_active() == self.player:\n",
    "                    self.set_field(field)\n",
    "                if not self.policies == None and not self.get_active() == self.player:\n",
    "                    self.step()\n",
    "    \n",
    "    def set_field(self, field: int) -> None:\n",
    "        # Set field to current\n",
    "        self.board[field] = self.get_active()\n",
    "\n",
    "        # Update the board\n",
    "        self.update()\n",
    "    \n",
    "    def get_winnable_fields(self, player: Type) -> List[int]:\n",
    "        # If there are 2 matching player type and 1 matching None return the fields\n",
    "        result = []\n",
    "        for row in rows:\n",
    "            fields = [[c, self.board[c]] for c in row]\n",
    "            noneMatches = [f for f in fields if f[1] == None]\n",
    "            playerMatches = [f for f in fields if f[1] == player]\n",
    "            if len(noneMatches) == 1 and len(playerMatches) == 2:\n",
    "                result.append(noneMatches[0][0])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def update(self) -> None:\n",
    "        if hasattr(self, \"field_buttons\"):\n",
    "            for i in range(len(self.field_buttons)):\n",
    "                self.field_buttons[i].description = (\n",
    "                    self.board[i].name if not self.board[i] == None else \" \"\n",
    "                )\n",
    "            if not self.policies == None:\n",
    "                self.player_select_button.description = \"PLAYER \" + self.player.name\n",
    "            result = self.get_result()\n",
    "            if not result == None:\n",
    "                self.result_text.value = \"winner is <b>{}</b>\".format(result.name)\n",
    "            else:\n",
    "                self.result_text.value = \"\"\n",
    "\n",
    "    def render(self) -> None:\n",
    "        if not hasattr(self, \"field_buttons\"):\n",
    "            self.field_buttons = []\n",
    "            elements = []\n",
    "\n",
    "            # Add header\n",
    "            elements.append(HTML(value=\"<h1>TicTacToe</h1>\"))\n",
    "\n",
    "            # Add field buttons\n",
    "            buttons = []\n",
    "            rows = []\n",
    "            for i in range(9):\n",
    "                btn = Button(layout=field_layout)\n",
    "                btn.on_click(functools.partial(self.field_click, i))\n",
    "                buttons.append(btn)\n",
    "                self.field_buttons.append(btn)\n",
    "                if (i + 1) % 3 == 0:\n",
    "                    rows.append(Box(buttons))\n",
    "                    buttons = []\n",
    "            elements.append(Box(children=rows, layout=column_layout))\n",
    "            \n",
    "            # Add player select\n",
    "            if not self.policies == None and not len(self.policies) > 1:\n",
    "                player_btn = Button(\n",
    "                    description=\"PLAYER \" + self.player.name, layout=wide_layout\n",
    "                )\n",
    "                player_btn.on_click(self.change_player)\n",
    "                self.player_select_button = player_btn\n",
    "\n",
    "                elements.append(player_btn)\n",
    "\n",
    "            # Add reset button\n",
    "            reset_btn = Button(description=\"RESET\", layout=wide_layout)\n",
    "            reset_btn.on_click(self.reset)\n",
    "            elements.append(reset_btn)\n",
    "\n",
    "            # Add result text\n",
    "            result_text = HTML()\n",
    "            elements.append(result_text)\n",
    "            self.result_text = result_text\n",
    "\n",
    "            # Display elements and data\n",
    "            display(Box(children=elements, layout=column_layout))\n",
    "        \n",
    "        # Update the board\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67524fb2",
   "metadata": {},
   "source": [
    "## 3. No Agent (player vs player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing environment and rendering\n",
    "tictactoe = TicTacToeEnvironment()\n",
    "tictactoe.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974643b0",
   "metadata": {},
   "source": [
    "## 4. Unbeatable computer\n",
    "\n",
    "An unbeatable computer that consistantly plays the same best moves, sometimes there are multiple equally good moves, then it takes the first one of these moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21611412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computed_best_policy(tictactoe: TicTacToeEnvironment) -> int:\n",
    "    player = tictactoe.get_active()\n",
    "    opponent = Type.X if player == Type.O else Type.O\n",
    "\n",
    "    # If player is able to make 3 in a row, place to win\n",
    "    fields = tictactoe.get_winnable_fields(player)\n",
    "    if len(fields) > 0:\n",
    "        return fields[0]\n",
    "\n",
    "    # If opponent is able to make 3 in a row, place to prevent\n",
    "    fields = tictactoe.get_winnable_fields(opponent)\n",
    "    if len(fields) > 0:\n",
    "        return fields[0]\n",
    "\n",
    "    # If center is empty, place in center\n",
    "    if tictactoe.board[4] == None:\n",
    "        return 4\n",
    "\n",
    "    # If a corner is empty and a corner on the opposite diagonal side is also empty, place in this first empty corner\n",
    "    empty_diagonal_corners = [i[0] for i in [[0, 8], [2, 6]] if tictactoe.board[i[0]] == None and tictactoe.board[i[1]] == None]\n",
    "    if len(empty_diagonal_corners) > 0:\n",
    "        return empty_diagonal_corners[0]\n",
    "\n",
    "    # If a corner is empty, place in first empty corner\n",
    "    empty_corners = [i for i in [0, 2, 6, 8] if tictactoe.board[i] == None]\n",
    "    if len(empty_corners) > 0:\n",
    "        return empty_corners[0]\n",
    "\n",
    "    # Place in first empty field\n",
    "    empty_fields = [[i, tictactoe.board[i]] for i in range(len(tictactoe.board)) if tictactoe.board[i] == None]\n",
    "    if len(empty_fields) > 0:\n",
    "        return empty_fields[0][0]\n",
    "\n",
    "# Initializing environment and rendering\n",
    "tictactoe = TicTacToeEnvironment([computed_best_policy])\n",
    "tictactoe.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a8aa0",
   "metadata": {},
   "source": [
    "## 5. Random Agent\n",
    "\n",
    "Two agents play against each other.\n",
    "\n",
    "Agent X: Uses the computed_best_policy.\n",
    "\n",
    "Agent O: Uses the random_policy.\n",
    "\n",
    "In the cell below, you can see the effect of an agent with a random policy choosing an arbitrary action regardless of the new state, playing against the computed_best_policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11aef49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate random move for agent\n",
    "def random_policy(tictactoe: TicTacToeEnvironment) -> int:\n",
    "    # Getting all the empty fields\n",
    "    empty_fields = [f for f in range(9) if tictactoe.board[f] == None]\n",
    "\n",
    "    # Choose random empty field\n",
    "    if len(empty_fields) > 0:\n",
    "        return random.choice(empty_fields)\n",
    "\n",
    "total_reward = 0.0\n",
    "\n",
    "tictactoe = TicTacToeEnvironment([computed_best_policy, random_policy])\n",
    "while tictactoe.get_result() == None and len([f for f in tictactoe.board if f == None]) > 0:\n",
    "    player, field, reward, state = tictactoe.step()\n",
    "    if player == Type.O:\n",
    "        total_reward += reward\n",
    "        print('action: {}\\tstate: {}, reward: {}'.format(player.name + ' ' + str(field), state, reward))\n",
    "    else:\n",
    "        print('action: {}\\tstate: {}'.format(player.name + ' ' + str(field), state))\n",
    "print('\\nEpisode done after {} steps. Total reward: {}'.format(tictactoe.get_turns(), total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9098173",
   "metadata": {},
   "source": [
    "If you run the cell above a number of times, you can observe two things:\n",
    "- The total reward and steps will vary from run to run.\n",
    "- O never wins, but sometimes the game ends in a tie.\n",
    "\n",
    "Each run from start state until stop state is called an episode.  \n",
    "Let's assemble some statistics on the episodes of the random agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae091479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "def run_one_episode(policy) -> int:\n",
    "    tictactoe = TicTacToeEnvironment([computed_best_policy, policy])\n",
    "    total_reward = 0.0\n",
    "    while tictactoe.get_result() == None and len([f for f in tictactoe.board if f == None]) > 0:\n",
    "        player, field, reward, state = tictactoe.step()\n",
    "        if player == Type.O:\n",
    "            total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "def measure_performance(policy, nr_episodes = 100):\n",
    "    N = nr_episodes\n",
    "    print('statistics over', N, 'episodes')\n",
    "    all_rewards = []\n",
    "    for _ in range(N):\n",
    "        episode_reward = run_one_episode(policy)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "    print('mean: {:6.2f}, sigma: {:6.2f}'.format(mean(all_rewards), stdev(all_rewards)))\n",
    "    print()\n",
    "    for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "    print('......')\n",
    "    for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards)-5):\n",
    "        print('ep: {:2d}, total reward: {:5.2f}'.format(n, episode_reward))\n",
    "\n",
    "measure_performance(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717bc2cb",
   "metadata": {},
   "source": [
    "## 5. Decisions based on reward agent\n",
    "\n",
    "Next we have the code for the agent that makes the decision based on reward. Since, we are working with a **Markov Decision Process** (MDP), we also have to define a reward function $R(s,a)$ that returns a value based on performing action $a$ in state $s$ both of which have been previously defined. Usually, we also define a transition function $T(s,a,s')$ that gives the probability of moving from a state $s$ to $s'$ when performing action $a$. This is only used in an environment where we are unsure or where it is unclear if some action always gives a predictable outcome. However, since tic-tac-toe is a simple game where every action is defined, we won't be needing a transition function.\n",
    "\n",
    "We will define the reward function $R(s,a)$ as follows:\n",
    "- Trivially, if the agent performs some action $â$ that wins the game from $s$, then $R(s,â) = 1$.\n",
    "- If the agent makes a mistake where the wrong action $ã$ loses the game, we then say $R(s,ã) = -1$.\n",
    "- When nothing happens we can simply say $R(s,a)=0$.\n",
    "\n",
    "We will be using Q-learning to find an optimal policy that the agent uses to decide which actions to pick. We will simply denote our policy as the action $a$ that maximises a function $Q(s,a)$ when the agent is in some state $s$. So, we would have something like:\n",
    "\n",
    "$$a^{best} = \\text{arg}\\max_{a\\in A}Q(s, a)$$\n",
    "\n",
    "For every state, each action has an associated value of $Q$ and we want to pick the $Q$ with the highest value. So, to compute $Q(s,a)$, the agent has to go over each possible pairs of states and actions while getting feedback from the reward function. We will update $Q(s,a)$ iteratively by letting the agent play. We will update $Q$ as follows:\n",
    "\n",
    "$$Q(s,a)^{new} \\leftarrow (1 - \\alpha)\\cdot Q(s,a)+\\alpha\\cdot(R(s,a)+\\gamma\\cdot\\max_{â\\in A}Q(ŝ, â))$$\n",
    "\n",
    "- We perform an action $a$ in the current state $s$.\n",
    "- $\\max_{â\\in A}Q(ŝ, â))$ takes into account future states and returns the largest $Q$, ŝ is the state that is the new state after performing $a$. Then $â$ is the best action.\n",
    "- $\\alpha$ is the learning rate that decides to what extent we overwrite the old value, we will use $\\alpha=0.1$.\n",
    "- The discount factor $\\gamma$ decides how much future rewards should be weighted compared to present rewards at the current time step $t$. We will be using $\\gamma=0.9$.\n",
    "\n",
    "We will find (read: learn) the best values for $Q(s,a)$. This will be done by letting two agents play against each other. To make sure it is balanced and that agents also seek out new options we will introduce a probability $\\epsilon$ that an agent picks a random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4068e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the best move for agent based on reward\n",
    "def reward_based_move(tictactoe: TicTacToeEnvironment) -> None:\n",
    "    # Getting all the empty fields\n",
    "    empty_fields = [\n",
    "        [f, 0.0] for f in range(len(tictactoe.board)) if tictactoe.board[f] == None\n",
    "    ]\n",
    "\n",
    "    if len(empty_fields) > 0:\n",
    "        # Calculate reward for empty fields\n",
    "\n",
    "        # Choose field with highest reward\n",
    "        highest_reward = max([x[1] for x in empty_fields])\n",
    "        highest_field = random.choice(\n",
    "            [field for field in empty_fields if field[1] == highest_reward]\n",
    "        )\n",
    "        return highest_field[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b495613",
   "metadata": {},
   "source": [
    "## Sources\n",
    "- For reinforcement learning (Q-learning): https://towardsdatascience.com/how-to-play-tic-tac-toe-using-reinforcement-learning-9604130e56f6"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21cebe2f5e3224dba0c406142577afe24fece8cdabfe36ea782fc3cbe70ab6c4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
